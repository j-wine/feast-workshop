{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1: Serving fresh online features with Feast, Kafka, Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "In this notebook, we explore why it's valuable to build / register streaming features in Feast for both data scientists and engineers. \n",
    "\n",
    "We then use Spark to build streaming features from events in Kafka and registering them within Feast. We then showcase how Feast combines these streaming features with batch data sources in the online store (Redis). Users can then retrieve features at low latency from Redis through Feast.\n",
    "\n",
    "If you haven't already, look at the [README](../README.md) for setup instructions prior to starting this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../architecture.png\" width=\"750\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup Spark Structured Streaming to read this Kafka Topic\n",
    "We first read and parse events from Kafka, run some transformations with Spark, and `forEachBatch` push to Feast"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:17:58.841263Z",
     "start_time": "2024-12-08T21:17:58.532568Z"
    }
   },
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"--packages=org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0 pyspark-shell\""
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:18:15.482028Z",
     "start_time": "2024-12-08T21:18:00.708745Z"
    }
   },
   "source": [
    "spark = SparkSession.builder.master(\"local\").appName(\"feast-spark\").getOrCreate()\n",
    "# Reduce partitions since default is 200 which will be slow on a local machine\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 5)\n",
    "\n",
    "schema = (\n",
    "    StructType()\n",
    "        .add('driver_id', IntegerType(), False)\n",
    "        .add('miles_driven', DoubleType(), False)\n",
    "        .add('event_timestamp', TimestampType(), False)\n",
    "        .add('conv_rate', DoubleType(), False)\n",
    "        .add('acc_rate', DoubleType(), False)\n",
    ")\n",
    "\n",
    "# Subscribe to 1 topic, with headers\n",
    "df = (\n",
    "    spark.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\n",
    "    .option(\"subscribe\", \"drivers\")\n",
    "    .option(\"startingOffsets\", \"earliest\")\n",
    "    .load()\n",
    "    .selectExpr('CAST(value AS STRING)')\n",
    "    .select(from_json('value', schema).alias(\"temp\"))\n",
    "    .select(\"temp.*\")\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Setup the feature store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply feature repository\n",
    "We first run `feast apply` to register the data sources + features and setup Redis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:48:31.153490Z",
     "start_time": "2024-12-08T17:48:20.167057Z"
    }
   },
   "source": [
    "!feast apply"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No project found in the repository. Using project name feast_demo_local defined in feature_store.yaml\n",
      "Applying changes for project feast_demo_local\n",
      "Deploying infrastructure for driver_hourly_stats\n",
      "Deploying infrastructure for driver_daily_features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/08/2024 06:48:28 PM feast.repo_config WARNING: The `path` of the `RegistryConfig` starts with a plain `postgresql` string. We are updating this to `postgresql+psycopg` to ensure that the `psycopg3` driver is used by `sqlalchemy`. If you want to use `psycopg2` pass `postgresql+psycopg2` explicitely to `path`. To silence this warning, pass `postgresql+psycopg` explicitely to `path`.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate a Feast `FeatureStore` object to push data to"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:17:28.103301Z",
     "start_time": "2024-12-08T20:17:27.673287Z"
    }
   },
   "source": [
    "from feast import FeatureStore\n",
    "from datetime import datetime\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `path` of the `RegistryConfig` starts with a plain `postgresql` string. We are updating this to `postgresql+psycopg` to ensure that the `psycopg3` driver is used by `sqlalchemy`. If you want to use `psycopg2` pass `postgresql+psycopg2` explicitely to `path`. To silence this warning, pass `postgresql+psycopg` explicitely to `path`.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch training data from offline store\n",
    "Just to verify the features are in the batch sources."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:15:02.527670800Z",
     "start_time": "2024-12-08T17:53:07.084451Z"
    }
   },
   "source": [
    "entity_df = pd.DataFrame.from_dict(\n",
    "    {\n",
    "        \"driver_id\": [1001, 1002, 1003, 1004, 1001],\n",
    "        \"event_timestamp\": [\n",
    "            datetime(2021, 4, 12, 10, 59, 42),\n",
    "            datetime(2021, 4, 12, 8, 12, 10),\n",
    "            datetime(2021, 4, 12, 16, 40, 26),\n",
    "            datetime(2021, 4, 12, 15, 1, 12),\n",
    "            datetime.now()\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\"\n",
    "    ],\n",
    ").to_df()\n",
    "print(training_df.head())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   driver_id                  event_timestamp  conv_rate  acc_rate  \\\n",
      "0       1001        2021-04-12 10:59:42+00:00   0.521149  0.751659   \n",
      "1       1002        2021-04-12 08:12:10+00:00   0.089014  0.212637   \n",
      "2       1003        2021-04-12 16:40:26+00:00   0.188855  0.344736   \n",
      "3       1004        2021-04-12 15:01:12+00:00   0.296492  0.935305   \n",
      "4       1001 2024-12-08 18:53:07.087475+00:00   0.404588  0.407571   \n",
      "\n",
      "   daily_miles_driven  \n",
      "0           18.926695  \n",
      "1           12.005569  \n",
      "2           23.490234  \n",
      "3           19.204191  \n",
      "4          350.650257  \n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Materialize batch features & fetch online features from Redis\n",
    "First we materialize features (which generate the latest values for each entity key from batch sources) into the online store (Redis)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:56:40.316388Z",
     "start_time": "2024-12-08T17:56:34.981914Z"
    }
   },
   "cell_type": "code",
   "source": "!feast materialize-incremental $(date +\"%Y-%m-%d\")\n",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: feast materialize-incremental [OPTIONS] END_TS\n",
      "Try 'feast materialize-incremental --help' for help.\n",
      "\n",
      "Error: Got unexpected extra argument (+%Y-%m-%d))\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T17:53:18.046756Z",
     "start_time": "2024-12-08T17:53:12.496829Z"
    }
   },
   "source": [
    "!feast materialize-incremental $(date +%Y-%m-%d)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Usage: feast materialize-incremental [OPTIONS] END_TS\n",
      "Try 'feast materialize-incremental --help' for help.\n",
      "\n",
      "Error: Got unexpected extra argument (+%Y-%m-%d))\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T18:01:09.151574Z",
     "start_time": "2024-12-08T18:01:02.627571Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datetime import datetime, timezone\n",
    "\n",
    "# Get today's date in UTC and convert it to a string\n",
    "end_ts = datetime.now(timezone.utc).strftime('%Y-%m-%dT%H:%M:%S%z')\n",
    "\n",
    "# Construct and run the command\n",
    "import subprocess\n",
    "\n",
    "command = f\"feast materialize-incremental {end_ts}\"\n",
    "try:\n",
    "    result = subprocess.run(command, shell=True, check=True, text=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(result.stdout)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Error occurred: {e}\")\n",
    "    print(f\"Standard Output:\\n{e.stdout}\")\n",
    "    print(f\"Standard Error:\\n{e.stderr}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred: Command 'feast materialize-incremental 2024-12-08T18:01:02+0000' returned non-zero exit status 1.\n",
      "Standard Output:\n",
      "Materializing \u001B[1m\u001B[32m2\u001B[0m feature views to \u001B[1m\u001B[32m2024-12-08 19:01:02+01:00\u001B[0m into the \u001B[1m\u001B[32mredis\u001B[0m online store.\n",
      "\n",
      "\n",
      "Standard Error:\n",
      "12/08/2024 07:01:08 PM feast.repo_config WARNING: The `path` of the `RegistryConfig` starts with a plain `postgresql` string. We are updating this to `postgresql+psycopg` to ensure that the `psycopg3` driver is used by `sqlalchemy`. If you want to use `psycopg2` pass `postgresql+psycopg2` explicitely to `path`. To silence this warning, pass `postgresql+psycopg` explicitely to `path`.\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Scripts\\feast.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\click\\core.py\", line 1157, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\click\\core.py\", line 1078, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\click\\core.py\", line 1688, in invoke\n",
      "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\click\\core.py\", line 1434, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\click\\core.py\", line 783, in invoke\n",
      "    return __callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\click\\decorators.py\", line 33, in new_func\n",
      "    return f(get_current_context(), *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\feast\\cli.py\", line 816, in materialize_incremental_command\n",
      "    store.materialize_incremental(\n",
      "  File \"C:\\Users\\jofwf\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\feast\\feature_store.py\", line 1337, in materialize_incremental\n",
      "    f\" from {Style.BRIGHT + Fore.GREEN}{start_date.replace(microsecond=0).astimezone()}{Style.RESET_ALL}\"\n",
      "                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "OSError: [Errno 22] Invalid argument\n",
      "\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feast manages what time intervals have been materialized in the registry. So if you schedule regular materialization every hour, you can run `feast materialize-incremental` and Feast will know that all the previous hours were already processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SDK based online retrieval\n",
    "Now we can retrieve these materialized features from Redis by directly using the SDK. This is one of the most popular ways to retrieve features with Feast since it allows you to integrate with an existing service (e.g. a Flask) that also handles model inference or pre/post-processing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:17:50.004083Z",
     "start_time": "2024-12-08T20:17:49.987244Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "\n",
    "def print_online_features(features):\n",
    "    for key, value in sorted(features.items()):\n",
    "        print(key, \" : \", value)\n",
    "\n",
    "print_online_features(features)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [None]\n",
      "conv_rate  :  [None]\n",
      "daily_miles_driven  :  [None]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HTTP based online retrieval\n",
    "We can also retrieve from a deployed feature server. We had previously deployed this with Docker Compose (see [docker-compose.yml](../docker-compose.yml))\n",
    "\n",
    "This can be preferable for many reasons. If you want to build an in-memory cache, caching on a central feature server can allow more effective caching across teams. You can also more centrally manage rate-limiting / access control, upgrade Feast versions independently, etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:20:19.160706Z",
     "start_time": "2024-12-08T20:20:19.118570Z"
    }
   },
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "online_request = {\n",
    "  \"features\": [\n",
    "    \"driver_hourly_stats:conv_rate\",\n",
    "    \"driver_hourly_stats:acc_rate\"\n",
    "  ],\n",
    "  \"entities\": {\n",
    "    \"driver_id\": [1001]\n",
    "  }\n",
    "}\n",
    "r = requests.post('http://localhost:6566/get-online-features', data=json.dumps(online_request))\n",
    "print(json.dumps(r.json(), indent=4, sort_keys=True))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"metadata\": {\n",
      "        \"feature_names\": [\n",
      "            \"driver_id\",\n",
      "            \"acc_rate\",\n",
      "            \"conv_rate\"\n",
      "        ]\n",
      "    },\n",
      "    \"results\": [\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"1970-01-01T00:00:00Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                1001\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"1970-01-01T00:00:00Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                null\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"event_timestamps\": [\n",
      "                \"1970-01-01T00:00:00Z\"\n",
      "            ],\n",
      "            \"statuses\": [\n",
      "                \"PRESENT\"\n",
      "            ],\n",
      "            \"values\": [\n",
      "                null\n",
      "            ]\n",
      "        }\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generating fresher features via stream transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Building streaming features with Kafka + Spark Structured Streaming\n",
    "Now we push streaming features into Feast by ingesting events from Kafka and processing with Spark Structured Streaming.\n",
    "- These features can then be further post-processed and combined with other features or request data in on demand transforms.\n",
    "- An example might be to push in the last 5 transactions, and in on demand transforms generate the average of those transactions.\n",
    "\n",
    "Feast will help manage both batch and streaming sources for you. You can run `feast materialize-incremental` as well as ingest streaming features to the same online store.\n",
    "\n",
    "**Below, what's happening:**\n",
    "- We use Spark to compute a sliding window aggregate feature that computes `daily_miles_driven` using the `miles_driven` column in the event.\n",
    "- Triggers every 30 seconds\n",
    "    - In this case, because we’re just reading from the `driver_stats.parquet`, we could have multiple windows of data coming in. Thus, in this code, we filter for the latest (`driver_id`, `window`) feature\n",
    "    - If you have a larger watermark and can get events across multiple windows, you’ll want to have the latest window too.\n",
    "- Rename `end` to `event_timestamp`, otherwise Feast will throw a validation error since it doesn’t match the schema of the `FeatureView`"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T21:21:19.835262Z",
     "start_time": "2024-12-08T21:21:16.108759Z"
    }
   },
   "source": [
    "def send_to_feast(df, epoch):\n",
    "    pandas_df: pd.DataFrame = df.toPandas()\n",
    "    if pandas_df.empty:\n",
    "        return\n",
    "    \n",
    "    if \"end\" in pandas_df:\n",
    "        print(\"processing window\")\n",
    "        # Filter out only for the latest window for the driver id\n",
    "        pandas_df = pandas_df.sort_values(by=[\"driver_id\",\"end\"], ascending=False).groupby(\"driver_id\").nth(-1)\n",
    "        pandas_df = pandas_df.rename(columns = {\"end\": \"event_timestamp\"})\n",
    "        pandas_df['created'] = pd.to_datetime('now', utc=True)\n",
    "        store.push(\"driver_stats_push_source\", pandas_df)\n",
    "    pandas_df.sort_values(by=\"driver_id\", inplace=True)\n",
    "    print(pandas_df.head(20))\n",
    "    print(f\"Num rows: {len(pandas_df.index)}\")\n",
    "\n",
    "daily_miles_driven = (\n",
    "        df.withWatermark(\"event_timestamp\", \"1 second\")\n",
    "        .groupBy(\"driver_id\", window(timeColumn=\"event_timestamp\", windowDuration=\"1 day\", slideDuration=\"1 hour\"))\n",
    "        .agg(sum(\"miles_driven\").alias(\"daily_miles_driven\"))\n",
    "        .select(\"driver_id\", \"window.end\", \"daily_miles_driven\")\n",
    ")\n",
    "\n",
    "query_1 = daily_miles_driven \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/feast-workshop/q1/\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .foreachBatch(send_to_feast) \\\n",
    "    .start()\n",
    "\n",
    "query_1.awaitTermination(timeout=30)\n",
    "query_1.stop()"
   ],
   "outputs": [
    {
     "ename": "StreamingQueryException",
     "evalue": "[STREAM_FAILED] Query [id = 333ec1ad-c0bf-4003-ab92-65d3c8efc95f, runId = 9a5aa5c3-b6ae-4469-9ecc-2c7331d9068a] terminated with exception: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[19], line 32\u001B[0m\n\u001B[0;32m     17\u001B[0m daily_miles_driven \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m     18\u001B[0m         df\u001B[38;5;241m.\u001B[39mwithWatermark(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1 second\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     19\u001B[0m         \u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, window(timeColumn\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mevent_timestamp\u001B[39m\u001B[38;5;124m\"\u001B[39m, windowDuration\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1 day\u001B[39m\u001B[38;5;124m\"\u001B[39m, slideDuration\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m1 hour\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     20\u001B[0m         \u001B[38;5;241m.\u001B[39magg(\u001B[38;5;28msum\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmiles_driven\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39malias(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdaily_miles_driven\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m     21\u001B[0m         \u001B[38;5;241m.\u001B[39mselect(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_id\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwindow.end\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdaily_miles_driven\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     22\u001B[0m )\n\u001B[0;32m     24\u001B[0m query_1 \u001B[38;5;241m=\u001B[39m daily_miles_driven \\\n\u001B[0;32m     25\u001B[0m     \u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[0;32m     26\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mupdate\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;241m.\u001B[39mforeachBatch(send_to_feast) \\\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m---> 32\u001B[0m \u001B[43mquery_1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m30\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     33\u001B[0m query_1\u001B[38;5;241m.\u001B[39mstop()\n",
      "File \u001B[1;32m~\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\pyspark\\sql\\streaming\\query.py:219\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[1;34m(self, timeout)\u001B[0m\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(timeout, (\u001B[38;5;28mint\u001B[39m, \u001B[38;5;28mfloat\u001B[39m)) \u001B[38;5;129;01mor\u001B[39;00m timeout \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    215\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m PySparkValueError(\n\u001B[0;32m    216\u001B[0m             error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mVALUE_NOT_POSITIVE\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    217\u001B[0m             message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtimeout\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_value\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(timeout)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[0;32m    218\u001B[0m         )\n\u001B[1;32m--> 219\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mint\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    220\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
      "File \u001B[1;32m~\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\PycharmProjects\\feast-workshop\\venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:185\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    181\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[0;32m    182\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[1;32m--> 185\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    186\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    187\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
      "\u001B[1;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = 333ec1ad-c0bf-4003-ab92-65d3c8efc95f, runId = 9a5aa5c3-b6ae-4469-9ecc-2c7331d9068a] terminated with exception: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5b. Verify fresh features\n",
    "Now we can verify that the `daily_miles_driven` feature has indeed changed from the original materialized features."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-08T20:17:38.872356Z",
     "start_time": "2024-12-08T20:17:38.815598Z"
    }
   },
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "print_online_features(features)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_online_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 13\u001B[0m\n\u001B[0;32m      1\u001B[0m features \u001B[38;5;241m=\u001B[39m store\u001B[38;5;241m.\u001B[39mget_online_features(\n\u001B[0;32m      2\u001B[0m     features\u001B[38;5;241m=\u001B[39m[\n\u001B[0;32m      3\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdriver_hourly_stats:conv_rate\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     11\u001B[0m     ],\n\u001B[0;32m     12\u001B[0m )\u001B[38;5;241m.\u001B[39mto_dict()\n\u001B[1;32m---> 13\u001B[0m \u001B[43mprint_online_features\u001B[49m(features)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'print_online_features' is not defined"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5c (optional): Push features through the push server\n",
    "We could also have pushed features without a `FeatureStore` object (and hence the Spark jobs don't need a `feature_store.yaml`) and instead through an HTTP request to the push server (`requests.post(\"http://localhost:6567/push\", data=json.dumps(push_data))`) \n",
    "\n",
    "We already deployed a push server with Docker Compose at port 6567. Separating writes (pushes) from reads (feature serving) can be helpful for independent scaling + resource isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/18 12:23:06 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "22/05/18 12:23:06 WARN StreamingQueryManager: Stopping existing streaming query [id=c1245bc6-365d-4770-9172-1fb403ccb4f6, runId=61ed0b3f-2948-4170-8187-5ffd4e577502], as a new run is being started.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n",
      "22/05/18 12:23:10 WARN HDFSBackedStateStoreProvider: The state for version 2 doesn't exist in loadedMaps. Reading snapshot file and delta files if needed...Note that this is normal for the first batch of starting query.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing window\n",
      "   driver_id      event_timestamp  daily_miles_driven  \\\n",
      "0       1001  2023-03-08 13:00:00          596.305352   \n",
      "1       1002  2023-03-08 14:00:00          644.051849   \n",
      "2       1003  2023-03-08 14:00:00          547.179926   \n",
      "3       1004  2023-03-08 14:00:00          695.276622   \n",
      "4       1005  2023-03-08 14:00:00          640.746719   \n",
      "\n",
      "                            created  \n",
      "0  2022-05-18 16:23:10.699456+00:00  \n",
      "1  2022-05-18 16:23:10.699456+00:00  \n",
      "2  2022-05-18 16:23:10.699456+00:00  \n",
      "3  2022-05-18 16:23:10.699456+00:00  \n",
      "4  2022-05-18 16:23:10.699456+00:00  \n",
      "Num rows: 5\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def send_to_feast(df, epoch):\n",
    "    pandas_df: pd.DataFrame = df.toPandas()\n",
    "    if pandas_df.empty:\n",
    "        return\n",
    "    \n",
    "    if \"end\" in pandas_df:\n",
    "        print(\"processing window\")\n",
    "        # Filter out only for the latest window for the driver id\n",
    "        pandas_df = pandas_df.sort_values(by=[\"driver_id\",\"end\"], ascending=False).groupby(\"driver_id\").nth(-1)\n",
    "        pandas_df = pandas_df.rename(columns = {\"end\": \"event_timestamp\"})\n",
    "        pandas_df['created'] = pd.to_datetime('now', utc=True)\n",
    "        pandas_df.reset_index(inplace=True)\n",
    "\n",
    "        # Push the event via HTTP (we need to map timestamps to strings since timestamps aren't serializable)\n",
    "        pandas_df['created'] = pandas_df['created'].astype(str)\n",
    "        pandas_df['event_timestamp'] = pandas_df['event_timestamp'].astype(str)\n",
    "        pandas_dict = pandas_df.to_dict(orient=\"list\")\n",
    "        push_data = {\n",
    "            \"push_source_name\":\"driver_stats_push_source\",\n",
    "            \"df\":pandas_dict\n",
    "        }\n",
    "        requests.post(\"http://localhost:6567/push\", data=json.dumps(push_data))\n",
    "    pandas_df.sort_values(by=\"driver_id\", inplace=True)\n",
    "    print(pandas_df.head(20))\n",
    "    print(f\"Num rows: {len(pandas_df.index)}\")\n",
    "\n",
    "query_2 = daily_miles_driven \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/feast-workshop/q2/\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .foreachBatch(send_to_feast) \\\n",
    "    .start()\n",
    "\n",
    "query_2.awaitTermination(timeout=20)\n",
    "query_2.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can verify yet again that features are more fresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc_rate  :  [0.4075707495212555]\n",
      "conv_rate  :  [0.4045884609222412]\n",
      "daily_miles_driven  :  [596.3053588867188]\n",
      "driver_id  :  [1001]\n"
     ]
    }
   ],
   "source": [
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"driver_hourly_stats:conv_rate\",\n",
    "        \"driver_hourly_stats:acc_rate\",\n",
    "        \"driver_daily_features:daily_miles_driven\",\n",
    "    ],\n",
    "    entity_rows=[\n",
    "        {\n",
    "            \"driver_id\": 1001,\n",
    "        }\n",
    "    ],\n",
    ").to_dict()\n",
    "print_online_features(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "Finally, let's clean up the checkpoint directories from Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "dir_path = '/tmp/feast-workshop/'\n",
    "\n",
    "try:\n",
    "    shutil.rmtree(dir_path)\n",
    "except OSError as e:\n",
    "    print(\"Error: %s : %s\" % (dir_path, e.strerror))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d634b9af180bcb32a446a43848522733ff8f5bbf0cc46dba1a83bede04bf237"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('python-3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
